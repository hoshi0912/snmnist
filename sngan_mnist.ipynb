{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sngan_mnist.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoshi0912/snmnist/blob/master/sngan_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfZW96vRmjWC",
        "colab_type": "code",
        "outputId": "e6734ed3-da07-4053-fd73-9f90b0fe293e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!pip install chainercv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: chainercv in /usr/local/lib/python3.6/dist-packages (0.13.0)\n",
            "Requirement already satisfied: chainer>=6.0 in /usr/local/lib/python3.6/dist-packages (from chainercv) (6.0.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from chainercv) (4.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from chainer>=6.0->chainercv) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from chainer>=6.0->chainercv) (3.7.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from chainer>=6.0->chainercv) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from chainer>=6.0->chainercv) (1.16.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from chainer>=6.0->chainercv) (41.0.1)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from chainer>=6.0->chainercv) (3.6.6)\n",
            "Requirement already satisfied: protobuf<3.8.0rc1,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from chainer>=6.0->chainercv) (3.7.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->chainercv) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIOe6hYX7OpC",
        "colab_type": "code",
        "outputId": "3b13e248-1866-4e94-806d-14ebc4b7d3bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!pip install 'cupy-cuda100>=6.0.0,<7.0.0'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cupy-cuda100<7.0.0,>=6.0.0 in /usr/local/lib/python3.6/dist-packages (6.0.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda100<7.0.0,>=6.0.0) (1.16.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda100<7.0.0,>=6.0.0) (1.12.0)\n",
            "Requirement already satisfied: fastrlock>=0.3 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda100<7.0.0,>=6.0.0) (0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LievEKFXPf2f",
        "colab_type": "code",
        "outputId": "e1b7b998-80b9-486f-e903-9b78985fd3a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy\n",
        "import numpy as np\n",
        "\n",
        "import chainer\n",
        "from chainer import cuda\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "from chainer import Variable\n",
        "from chainer.training import extensions\n",
        "from chainer import initializers\n",
        "from chainer import link\n",
        "from chainer.utils import argument\n",
        "from chainer import variable\n",
        "from chainer.links import EmbedID\n",
        "from chainer import configuration\n",
        "from chainer.functions.normalization import batch_normalization\n",
        "from chainer.functions.array.broadcast import broadcast_to\n",
        "from chainer.initializers import normal\n",
        "from chainer.functions.connection import embed_id\n",
        "from chainer.functions.connection import linear\n",
        "from chainer.links.connection.linear import Linear\n",
        "from chainer.functions.connection import convolution_2d\n",
        "from chainer.links.connection.convolution_2d import Convolution2D\n",
        "import math\n",
        "from chainercv import transforms\n",
        "\n",
        "chainer.print_runtime_info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Platform: Linux-4.14.79+-x86_64-with-Ubuntu-18.04-bionic\n",
            "Chainer: 6.0.0\n",
            "NumPy: 1.16.4\n",
            "CuPy:\n",
            "  CuPy Version          : 6.0.0\n",
            "  CUDA Root             : /usr/local/cuda\n",
            "  CUDA Build Version    : 10000\n",
            "  CUDA Driver Version   : 10000\n",
            "  CUDA Runtime Version  : 10000\n",
            "  cuDNN Build Version   : 7500\n",
            "  cuDNN Version         : 7500\n",
            "  NCCL Build Version    : 2402\n",
            "  NCCL Runtime Version  : 2402\n",
            "iDeep: 2.0.0.post3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzypAWA4QDXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameters\n",
        "n_epoch = 100  # number of epochs\n",
        "n_hidden = 128  # number of hidden units\n",
        "batchsize = 64  # minibatch size\n",
        "snapshot_interval = 1000  # number of iterations per snapshots\n",
        "display_interval = 100  # number of iterations per display the status\n",
        "gpu_id = 0\n",
        "out_dir = 'result'\n",
        "seed = 0  # random seed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHAHdU4J7B1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform(input, image_size=(32, 32)):\n",
        "    imgs, _ = input\n",
        "    imgs = transforms.resize(imgs, size=image_size)\n",
        "    imgs = np.concatenate((imgs, imgs, imgs), axis=0)\n",
        "    return imgs, _"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vim3C3aaZbbH",
        "colab_type": "code",
        "outputId": "f6337fc0-a2ef-4c1c-8ff2-a021be666eea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Load the CIFAR10 dataset if args.dataset is not specified\n",
        "train, _ = chainer.datasets.get_mnist(withlabel=True, ndim=3, scale=255.)\n",
        "train = chainer.datasets.TransformDataset(train, transform)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading from http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz...\n",
            "Downloading from http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz...\n",
            "Downloading from http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz...\n",
            "Downloading from http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRElu1xrc4XU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter = chainer.iterators.SerialIterator(train, batchsize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v7bvG9gGkSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SNLinear(Linear):\n",
        "    \"\"\"Linear layer with Spectral Normalization.\n",
        "    Args:\n",
        "        in_size (int): Dimension of input vectors. If ``None``, parameter\n",
        "            initialization will be deferred until the first forward datasets pass\n",
        "            at which time the size will be determined.\n",
        "        out_size (int): Dimension of output vectors.\n",
        "        wscale (float): Scaling factor of the weight matrix.\n",
        "        bias (float): Initial bias value.\n",
        "        nobias (bool): If ``True``, then this function does not use the bias.\n",
        "        initialW (2-D array): Initial weight value. If ``None``, then this\n",
        "            function uses to initialize ``wscale``.\n",
        "            May also be a callable that takes ``numpy.ndarray`` or\n",
        "            ``cupy.ndarray`` and edits its value.\n",
        "        initial_bias (1-D array): Initial bias value. If ``None``, then this\n",
        "            function uses to initialize ``bias``.\n",
        "            May also be a callable that takes ``numpy.ndarray`` or\n",
        "            ``cupy.ndarray`` and edits its value.\n",
        "        use_gamma (bool): If true, apply scalar multiplication to the \n",
        "            normalized weight (i.e. reparameterize).\n",
        "        Ip (int): The number of power iteration for calculating the spcetral \n",
        "            norm of the weights.\n",
        "        factor (float) : constant factor to adjust spectral norm of W_bar.\n",
        "    .. seealso:: :func:`~chainer.functions.linear`\n",
        "    Attributes:\n",
        "        W (~chainer.Variable): Weight parameter.\n",
        "        W_bar (~chainer.Variable): Spectrally normalized weight parameter.\n",
        "        b (~chainer.Variable): Bias parameter.\n",
        "        u (~numpy.array): Current estimation of the right largest singular vector of W.\n",
        "        (optional) gamma (~chainer.Variable): the multiplier parameter.\n",
        "        (optional) factor (float): constant factor to adjust spectral norm of W_bar.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_size, out_size, use_gamma=False, nobias=False,\n",
        "                 initialW=None, initial_bias=None, Ip=1, factor=None):\n",
        "        self.Ip = Ip\n",
        "        self.use_gamma = use_gamma\n",
        "        self.factor = factor\n",
        "        super(SNLinear, self).__init__(\n",
        "            in_size, out_size, nobias, initialW, initial_bias\n",
        "        )\n",
        "        self.u = np.random.normal(size=(1, out_size)).astype(dtype=\"f\")\n",
        "        self.register_persistent('u')\n",
        "\n",
        "    @property\n",
        "    def W_bar(self):\n",
        "        \"\"\"\n",
        "        Spectral Normalized Weight\n",
        "        \"\"\"\n",
        "        sigma, _u, _ = max_singular_value(self.W, self.u, self.Ip)\n",
        "        if self.factor:\n",
        "            sigma = sigma / self.factor\n",
        "        sigma = broadcast_to(sigma.reshape((1, 1)), self.W.shape)\n",
        "        self.u[:] = _u\n",
        "        if hasattr(self, 'gamma'):\n",
        "            return broadcast_to(self.gamma, self.W.shape) * self.W / sigma\n",
        "        else:\n",
        "            return self.W / sigma\n",
        "\n",
        "    def _initialize_params(self, in_size):\n",
        "        super(SNLinear, self)._initialize_params(in_size)\n",
        "        if self.use_gamma:\n",
        "            _, s, _ = np.linalg.svd(self.W.data)\n",
        "            with self.init_scope():\n",
        "                self.gamma = chainer.Parameter(s[0], (1, 1))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Applies the linear layer.\n",
        "        Args:\n",
        "            x (~chainer.Variable): Batch of input vectors.\n",
        "        Returns:\n",
        "            ~chainer.Variable: Output of the linear layer.\n",
        "        \"\"\"\n",
        "        if self.W.data is None:\n",
        "            self._initialize_params(x.size // x.shape[0])\n",
        "        return linear.linear(x, self.W_bar, self.b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK9BjBhbVJ5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConditionalBatchNormalization(chainer.Chain):\n",
        "    \"\"\"\n",
        "    Conditional Batch Normalization\n",
        "    Args:\n",
        "        size (int or tuple of ints): Size (or shape) of channel\n",
        "            dimensions.\n",
        "        n_cat (int): the number of categories of categorical variable.\n",
        "        decay (float): Decay rate of moving average. It is used on training.\n",
        "        eps (float): Epsilon value for numerical stability.\n",
        "        dtype (numpy.dtype): Type to use in computing.\n",
        "        use_gamma (bool): If ``True``, use scaling parameter. Otherwise, use\n",
        "            unit(1) which makes no effect.\n",
        "        use_beta (bool): If ``True``, use shifting parameter. Otherwise, use\n",
        "            unit(0) which makes no effect.\n",
        "    See: `Batch Normalization: Accelerating Deep Network Training by Reducing\\\n",
        "          Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`_\n",
        "    .. seealso::\n",
        "       :func:`~chainer.functions.batch_normalization`,\n",
        "       :func:`~chainer.functions.fixed_batch_normalization`\n",
        "    Attributes:\n",
        "        gamma (~chainer.Variable): Scaling parameter.\n",
        "        beta (~chainer.Variable): Shifting parameter.\n",
        "        avg_mean (numpy.ndarray or cupy.ndarray): Population mean.\n",
        "        avg_var (numpy.ndarray or cupy.ndarray): Population variance.\n",
        "        N (int): Count of batches given for fine-tuning.\n",
        "        decay (float): Decay rate of moving average. It is used on training.\n",
        "        eps (float): Epsilon value for numerical stability. This value is added\n",
        "            to the batch variances.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, n_cat, decay=0.9, eps=2e-5, dtype=numpy.float32):\n",
        "        super(ConditionalBatchNormalization, self).__init__()\n",
        "        self.avg_mean = numpy.zeros(size, dtype=dtype)\n",
        "        self.register_persistent('avg_mean')\n",
        "        self.avg_var = numpy.zeros(size, dtype=dtype)\n",
        "        self.register_persistent('avg_var')\n",
        "        self.N = 0\n",
        "        self.register_persistent('N')\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "        self.n_cat = n_cat\n",
        "\n",
        "    def __call__(self, x, gamma, beta, **kwargs):\n",
        "        \"\"\"__call__(self, x, c, finetune=False)\n",
        "        Invokes the forward propagation of BatchNormalization.\n",
        "        In training mode, the BatchNormalization computes moving averages of\n",
        "        mean and variance for evaluatino during training, and normalizes the\n",
        "        input using batch statistics.\n",
        "        .. warning::\n",
        "           ``test`` argument is not supported anymore since v2.\n",
        "           Instead, use ``chainer.using_config('train', train)``.\n",
        "           See :func:`chainer.using_config`.\n",
        "        Args:\n",
        "            x (Variable): Input variable.\n",
        "            gamma (Variable): Input variable of gamma of shape\n",
        "            finetune (bool): If it is in the training mode and ``finetune`` is\n",
        "                ``True``, BatchNormalization runs in fine-tuning mode; it\n",
        "                accumulates the input array to compute population statistics\n",
        "                for normalization, and normalizes the input using batch\n",
        "                statistics.\n",
        "        \"\"\"\n",
        "        argument.check_unexpected_kwargs(\n",
        "            kwargs, test='test argument is not supported anymore. '\n",
        "                         'Use chainer.using_config')\n",
        "        finetune, = argument.parse_kwargs(kwargs, ('finetune', False))\n",
        "        with cuda.get_device_from_id(self._device_id):\n",
        "            _gamma = variable.Variable(self.xp.ones(\n",
        "                self.avg_mean.shape, dtype=x.dtype))\n",
        "        with cuda.get_device_from_id(self._device_id):\n",
        "            _beta = variable.Variable(self.xp.zeros(\n",
        "                self.avg_mean.shape, dtype=x.dtype))\n",
        "        if configuration.config.train:\n",
        "            if finetune:\n",
        "                self.N += 1\n",
        "                decay = 1. - 1. / self.N\n",
        "            else:\n",
        "                decay = self.decay\n",
        "            ret = chainer.functions.batch_normalization(x, _gamma, _beta, eps=self.eps, running_mean=self.avg_mean,\n",
        "                                                        running_var=self.avg_var, decay=decay)\n",
        "        else:\n",
        "            # Use running average statistics or fine-tuned statistics.\n",
        "            mean = variable.Variable(self.avg_mean)\n",
        "            var = variable.Variable(self.avg_var)\n",
        "            ret = batch_normalization.fixed_batch_normalization(\n",
        "                x, _gamma, _beta, mean, var, self.eps)\n",
        "        shape = ret.shape\n",
        "        ndim = len(shape)\n",
        "        gamma = F.broadcast_to(F.reshape(gamma, list(gamma.shape) + [1] * (ndim - len(gamma.shape))), shape)\n",
        "        beta = F.broadcast_to(F.reshape(beta, list(beta.shape) + [1] * (ndim - len(beta.shape))), shape)\n",
        "        return gamma * ret + beta\n",
        "\n",
        "\n",
        "def start_finetuning(self):\n",
        "    \"\"\"Resets the population count for collecting population statistics.\n",
        "    This method can be skipped if it is the first time to use the\n",
        "    fine-tuning mode. Otherwise, this method should be called before\n",
        "    starting the fine-tuning mode again.\n",
        "    \"\"\"\n",
        "    self.N = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMgctWoUU5_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CategoricalConditionalBatchNormalization(ConditionalBatchNormalization):\n",
        "    \"\"\"\n",
        "    Conditional Batch Normalization\n",
        "    Args:\n",
        "        size (int or tuple of ints): Size (or shape) of channel\n",
        "            dimensions.\n",
        "        n_cat (int): the number of categories of categorical variable.\n",
        "        decay (float): Decay rate of moving average. It is used on training.\n",
        "        eps (float): Epsilon value for numerical stability.\n",
        "        dtype (numpy.dtype): Type to use in computing.\n",
        "        use_gamma (bool): If ``True``, use scaling parameter. Otherwise, use\n",
        "            unit(1) which makes no effect.\n",
        "        use_beta (bool): If ``True``, use shifting parameter. Otherwise, use\n",
        "            unit(0) which makes no effect.\n",
        "    See: `Batch Normalization: Accelerating Deep Network Training by Reducing\\\n",
        "          Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`_\n",
        "    .. seealso::\n",
        "       :func:`~chainer.functions.batch_normalization`,\n",
        "       :func:`~chainer.functions.fixed_batch_normalization`\n",
        "    Attributes:\n",
        "        gamma (~chainer.Variable): Scaling parameter.\n",
        "        beta (~chainer.Variable): Shifting parameter.\n",
        "        avg_mean (numpy.ndarray or cupy.ndarray): Population mean.\n",
        "        avg_var (numpy.ndarray or cupy.ndarray): Population variance.\n",
        "        N (int): Count of batches given for fine-tuning.\n",
        "        decay (float): Decay rate of moving average. It is used on training.\n",
        "        eps (float): Epsilon value for numerical stability. This value is added\n",
        "            to the batch variances.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, n_cat, decay=0.9, eps=2e-5, dtype=numpy.float32,\n",
        "                 initial_gamma=None, initial_beta=None):\n",
        "        super(CategoricalConditionalBatchNormalization, self).__init__(\n",
        "            size=size, n_cat=n_cat, decay=decay, eps=eps, dtype=dtype)\n",
        "\n",
        "        with self.init_scope():\n",
        "            if initial_gamma is None:\n",
        "                initial_gamma = 1\n",
        "            initial_gamma = initializers._get_initializer(initial_gamma)\n",
        "            initial_gamma.dtype = dtype\n",
        "            self.gammas = EmbedID(n_cat, size, initialW=initial_gamma)\n",
        "            if initial_beta is None:\n",
        "                initial_beta = 0\n",
        "            initial_beta = initializers._get_initializer(initial_beta)\n",
        "            initial_beta.dtype = dtype\n",
        "            self.betas = EmbedID(n_cat, size, initialW=initial_beta)\n",
        "\n",
        "    def __call__(self, x, c, finetune=False, **kwargs):\n",
        "        \"\"\"__call__(self, x, c, finetune=False)\n",
        "        Invokes the forward propagation of BatchNormalization.\n",
        "        In training mode, the BatchNormalization computes moving averages of\n",
        "        mean and variance for evaluatino during training, and normalizes the\n",
        "        input using batch statistics.\n",
        "        .. warning::\n",
        "           ``test`` argument is not supported anymore since v2.\n",
        "           Instead, use ``chainer.using_config('train', train)``.\n",
        "           See :func:`chainer.using_config`.\n",
        "        Args:\n",
        "            x (Variable): Input variable.\n",
        "            c (Variable): Input variable for conditioning gamma and beta\n",
        "            finetune (bool): If it is in the training mode and ``finetune`` is\n",
        "                ``True``, BatchNormalization runs in fine-tuning mode; it\n",
        "                accumulates the input array to compute population statistics\n",
        "                for normalization, and normalizes the input using batch\n",
        "                statistics.\n",
        "        \"\"\"\n",
        "        weights, = argument.parse_kwargs(kwargs, ('weights', None))\n",
        "        if c.ndim == 2 and weights is not None:\n",
        "            _gamma_c = self.gammas(c)\n",
        "            _beta_c = self.betas(c)\n",
        "            _gamma_c = F.broadcast_to(F.expand_dims(weights, 2), _gamma_c.shape) * _gamma_c \n",
        "            _beta_c = F.broadcast_to(F.expand_dims(weights, 2), _beta_c.shape) * _beta_c\n",
        "            gamma_c = F.sum(_gamma_c, 1) \n",
        "            beta_c = F.sum(_beta_c, 1)\n",
        "        else:\n",
        "            gamma_c = self.gammas(c)\n",
        "            beta_c = self.betas(c)\n",
        "        return super(CategoricalConditionalBatchNormalization, self).__call__(x, gamma_c, beta_c, **kwargs)\n",
        "\n",
        "\n",
        "def start_finetuning(self):\n",
        "    \"\"\"Resets the population count for collecting population statistics.\n",
        "    This method can be skipped if it is the first time to use the\n",
        "    fine-tuning mode. Otherwise, this method should be called before\n",
        "    starting the fine-tuning mode again.\n",
        "    \"\"\"\n",
        "    self.N = 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iZSGpenRcne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _upsample(x):\n",
        "    h, w = x.shape[2:]\n",
        "    return F.unpooling_2d(x, 2, outsize=(h * 2, w * 2))\n",
        "\n",
        "\n",
        "def upsample_conv(x, conv):\n",
        "    return conv(_upsample(x))\n",
        "\n",
        "\n",
        "class GBlock(chainer.Chain):\n",
        "    def __init__(self, in_channels, out_channels, hidden_channels=None, ksize=3, pad=1,\n",
        "                 activation=F.relu, upsample=False, n_classes=0):\n",
        "        super(GBlock, self).__init__()\n",
        "        initializer = chainer.initializers.GlorotUniform(math.sqrt(2))\n",
        "        initializer_sc = chainer.initializers.GlorotUniform()\n",
        "        self.activation = activation\n",
        "        self.upsample = upsample\n",
        "        self.learnable_sc = in_channels != out_channels or upsample\n",
        "        hidden_channels = out_channels if hidden_channels is None else hidden_channels\n",
        "        self.n_classes = n_classes\n",
        "        with self.init_scope():\n",
        "            self.c1 = L.Convolution2D(in_channels, hidden_channels, ksize=ksize, pad=pad, initialW=initializer)\n",
        "            self.c2 = L.Convolution2D(hidden_channels, out_channels, ksize=ksize, pad=pad, initialW=initializer)\n",
        "            if n_classes > 0:\n",
        "                self.b1 = CategoricalConditionalBatchNormalization(in_channels, n_cat=n_classes)\n",
        "                self.b2 = CategoricalConditionalBatchNormalization(hidden_channels, n_cat=n_classes)\n",
        "            else:\n",
        "                self.b1 = L.BatchNormalization(in_channels)\n",
        "                self.b2 = L.BatchNormalization(hidden_channels)\n",
        "            if self.learnable_sc:\n",
        "                self.c_sc = L.Convolution2D(in_channels, out_channels, ksize=1, pad=0, initialW=initializer_sc)\n",
        "\n",
        "    def residual(self, x, y=None, z=None, **kwargs):\n",
        "        h = x\n",
        "        h = self.b1(h, y, **kwargs) if y is not None else self.b1(h, **kwargs)\n",
        "        h = self.activation(h)\n",
        "        h = upsample_conv(h, self.c1) if self.upsample else self.c1(h)\n",
        "        h = self.b2(h, y, **kwargs) if y is not None else self.b2(h, **kwargs)\n",
        "        h = self.activation(h)\n",
        "        h = self.c2(h)\n",
        "        return h\n",
        "\n",
        "    def shortcut(self, x):\n",
        "        if self.learnable_sc:\n",
        "            x = upsample_conv(x, self.c_sc) if self.upsample else self.c_sc(x)\n",
        "            return x\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def __call__(self, x, y=None, z=None, **kwargs):\n",
        "        return self.residual(x, y, z, **kwargs) + self.shortcut(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBi239mYQCjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(chainer.Chain):\n",
        "\n",
        "    def __init__(self, n_hidden = 128, n_classes=10, bottom_width=8, bottom_height=4, ch=256, wscale=0.02, activation=F.relu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.n_hidden = n_hidden\n",
        "        self.ch = ch\n",
        "        self.bottom_width = bottom_width\n",
        "        self.bottom_height = bottom_height\n",
        "        self.activation = activation\n",
        "\n",
        "        with self.init_scope():\n",
        "            w = chainer.initializers.GlorotUniform()\n",
        "            self.l1 = L.Linear(n_hidden, (bottom_width * bottom_height) * ch, initialW=w)\n",
        "            self.block2 = GBlock(ch, ch, activation=activation, upsample=True, n_classes=n_classes)\n",
        "            self.block3 = GBlock(ch, ch, activation=activation, upsample=True, n_classes=n_classes)\n",
        "            self.block4 = GBlock(ch, ch, activation=activation, upsample=True, n_classes=n_classes)\n",
        "            self.block5 = GBlock(ch * 4, ch * 2, activation=activation, upsample=True, n_classes=n_classes)\n",
        "            self.block6 = GBlock(ch * 2, ch, activation=activation, upsample=True, n_classes=n_classes)\n",
        "            self.b7 = L.BatchNormalization(ch)\n",
        "            self.l7 = L.Convolution2D(ch, 3, ksize=3, stride=1, pad=1, initialW=w)\n",
        "\n",
        "    def make_hidden(self, batchsize):\n",
        "        #np.random.uniform(-1, 1, (batchsize, self.n_hidden, 1, 1)).astype(np.float32)\n",
        "        return np.random.randn(batchsize, n_hidden).astype(np.float32)\n",
        "\n",
        "    def __call__(self, z, c = None, **kwargs):\n",
        "        #h = F.reshape(F.relu(self.bn0(self.l0(z))),(len(z), -1, self.bottom_width, self.bottom_height))\n",
        "        h = self.l1(z)\n",
        "        h = F.reshape(h,(h.shape[0], -1, self.bottom_width, self.bottom_height))\n",
        "        h = self.block2(h, c, **kwargs)\n",
        "        h = self.block3(h, c, **kwargs)\n",
        "        h = self.block4(h, c, **kwargs)\n",
        "        #h = self.block5(h, c)\n",
        "        #h = self.block6(h, c)\n",
        "        h = self.b7(h)\n",
        "        h = self.activation(h)\n",
        "        h = F.tanh(self.l7(h))\n",
        "        return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRWiOnzYpeg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _l2normalize(v, eps=1e-12):\n",
        "    norm = cuda.reduce('T x', 'T out',\n",
        "                       'x * x', 'a + b', 'out = sqrt(a)', 0,\n",
        "                       'norm_sn')\n",
        "    div = cuda.elementwise('T x, T norm, T eps',\n",
        "                           'T out',\n",
        "                           'out = x / (norm + eps)',\n",
        "                           'div_sn')\n",
        "    return div(v, norm(v), eps)\n",
        "\n",
        "\n",
        "def max_singular_value(W, u=None, Ip=1):\n",
        "    \"\"\"\n",
        "    Apply power iteration for the weight parameter\n",
        "    \"\"\"\n",
        "    if not Ip >= 1:\n",
        "        raise ValueError(\"The number of power iterations should be positive integer\")\n",
        "\n",
        "    xp = cuda.get_array_module(W.data)\n",
        "    if u is None:\n",
        "        u = xp.random.normal(size=(1, W.shape[0])).astype(xp.float32)\n",
        "    _u = u\n",
        "    for _ in range(Ip):\n",
        "        _v = _l2normalize(xp.dot(_u, W.data), eps=1e-12)\n",
        "        _u = _l2normalize(xp.dot(_v, W.data.transpose()), eps=1e-12)\n",
        "    sigma = F.sum(F.linear(_u, F.transpose(W)) * _v)\n",
        "    return sigma, _u, _v\n",
        "\n",
        "\n",
        "def max_singular_value_fully_differentiable(W, u=None, Ip=1):\n",
        "    \"\"\"\n",
        "    Apply power iteration for the weight parameter (fully differentiable version)\n",
        "    \"\"\"\n",
        "    if not Ip >= 1:\n",
        "        raise ValueError(\"The number of power iterations should be positive integer\")\n",
        "\n",
        "    xp = cuda.get_array_module(W.data)\n",
        "    if u is None:\n",
        "        u = xp.random.normal(size=(1, W.shape[0])).astype(xp.float32)\n",
        "    _u = u\n",
        "    for _ in range(Ip):\n",
        "        _v = F.normalize(F.matmul(_u, W), eps=1e-12)\n",
        "        _u = F.normalize(F.matmul(_v, F.transpose(W)), eps=1e-12)\n",
        "    _u = F.matmul(_v, F.transpose(W))\n",
        "    norm = F.sqrt(F.sum(_u ** 2))\n",
        "    return norm, _l2normalize(_u.data), _v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Amo53HLpLZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SNEmbedID(link.Link):\n",
        "    \"\"\"Efficient linear layer for one-hot input.\n",
        "    This is a link that wraps the :func:`~chainer.functions.embed_id` function.\n",
        "    This link holds the ID (word) embedding matrix ``W`` as a parameter.\n",
        "    Args:\n",
        "        in_size (int): Number of different identifiers (a.k.a. vocabulary\n",
        "            size).\n",
        "        out_size (int): Size of embedding vector.\n",
        "        initialW (2-D array): Initial weight value. If ``None``, then the\n",
        "            matrix is initialized from the standard normal distribution.\n",
        "            May also be a callable that takes ``numpy.ndarray`` or\n",
        "            ``cupy.ndarray`` and edits its value.\n",
        "        ignore_label (int or None): If ``ignore_label`` is an int value,\n",
        "            ``i``-th column of return value is filled with ``0``.\n",
        "        Ip (int): The number of power iteration for calculating the spcetral\n",
        "            norm of the weights.\n",
        "        factor (float) : constant factor to adjust spectral norm of W_bar.\n",
        "    .. seealso:: :func:`chainer.functions.embed_id`\n",
        "    Attributes:\n",
        "        W (~chainer.Variable): Embedding parameter matrix.\n",
        "        W_bar (~chainer.Variable): Spectrally normalized weight parameter.\n",
        "        u (~numpy.array): Current estimation of the right largest singular vector of W.\n",
        "        (optional) gamma (~chainer.Variable): the multiplier parameter.\n",
        "        (optional) factor (float): constant factor to adjust spectral norm of W_bar.\n",
        "    \"\"\"\n",
        "\n",
        "    ignore_label = None\n",
        "\n",
        "    def __init__(self, in_size, out_size, initialW=None, ignore_label=None, Ip=1, factor=None):\n",
        "        super(SNEmbedID, self).__init__()\n",
        "        self.ignore_label = ignore_label\n",
        "        self.Ip = Ip\n",
        "        self.factor = factor\n",
        "        with self.init_scope():\n",
        "            if initialW is None:\n",
        "                initialW = normal.Normal(1.0)\n",
        "            self.W = variable.Parameter(initialW, (in_size, out_size))\n",
        "\n",
        "        self.u = np.random.normal(size=(1, in_size)).astype(dtype=\"f\")\n",
        "        self.register_persistent('u')\n",
        "\n",
        "    @property\n",
        "    def W_bar(self):\n",
        "        \"\"\"\n",
        "        Spectral Normalized Weight\n",
        "        \"\"\"\n",
        "        sigma, _u, _ = max_singular_value(self.W, self.u, self.Ip)\n",
        "        if self.factor:\n",
        "            sigma = sigma / self.factor\n",
        "        sigma = broadcast_to(sigma.reshape((1, 1)), self.W.shape)\n",
        "        self.u[:] = _u\n",
        "        return self.W / sigma\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Extracts the word embedding of given IDs.\n",
        "        Args:\n",
        "            x (~chainer.Variable): Batch vectors of IDs.\n",
        "        Returns:\n",
        "            ~chainer.Variable: Batch of corresponding embeddings.\n",
        "        \"\"\"\n",
        "        return embed_id.embed_id(x, self.W_bar, ignore_label=self.ignore_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ykqsePhTOBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SNConvolution2D(Convolution2D):\n",
        "    \"\"\"Two-dimensional convolutional layer with spectral normalization.\n",
        "    This link wraps the :func:`~chainer.functions.convolution_2d` function and\n",
        "    holds the filter weight and bias vector as parameters.\n",
        "    Args:\n",
        "        in_channels (int): Number of channels of input arrays. If ``None``,\n",
        "            parameter initialization will be deferred until the first forward\n",
        "            datasets pass at which time the size will be determined.\n",
        "        out_channels (int): Number of channels of output arrays.\n",
        "        ksize (int or pair of ints): Size of filters (a.k.a. kernels).\n",
        "            ``ksize=k`` and ``ksize=(k, k)`` are equivalent.\n",
        "        stride (int or pair of ints): Stride of filter applications.\n",
        "            ``stride=s`` and ``stride=(s, s)`` are equivalent.\n",
        "        pad (int or pair of ints): Spatial padding width for input arrays.\n",
        "            ``pad=p`` and ``pad=(p, p)`` are equivalent.\n",
        "        wscale (float): Scaling factor of the initial weight.\n",
        "        bias (float): Initial bias value.\n",
        "        nobias (bool): If ``True``, then this link does not use the bias term.\n",
        "        initialW (4-D array): Initial weight value. If ``None``, then this\n",
        "            function uses to initialize ``wscale``.\n",
        "            May also be a callable that takes ``numpy.ndarray`` or\n",
        "            ``cupy.ndarray`` and edits its value.\n",
        "        initial_bias (1-D array): Initial bias value. If ``None``, then this\n",
        "            function uses to initialize ``bias``.\n",
        "            May also be a callable that takes ``numpy.ndarray`` or\n",
        "            ``cupy.ndarray`` and edits its value.\n",
        "        use_gamma (bool): If true, apply scalar multiplication to the \n",
        "            normalized weight (i.e. reparameterize).\n",
        "        Ip (int): The number of power iteration for calculating the spcetral \n",
        "            norm of the weights.\n",
        "        factor (float) : constant factor to adjust spectral norm of W_bar.\n",
        "    .. seealso::\n",
        "       See :func:`chainer.functions.convolution_2d` for the definition of\n",
        "       two-dimensional convolution.\n",
        "    Attributes:\n",
        "        W (~chainer.Variable): Weight parameter.\n",
        "        W_bar (~chainer.Variable): Spectrally normalized weight parameter.\n",
        "        b (~chainer.Variable): Bias parameter.\n",
        "        u (~numpy.array): Current estimation of the right largest singular vector of W.\n",
        "        (optional) gamma (~chainer.Variable): the multiplier parameter.\n",
        "        (optional) factor (float): constant factor to adjust spectral norm of W_bar.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, ksize, stride=1, pad=0,\n",
        "                 nobias=False, initialW=None, initial_bias=None, use_gamma=False, Ip=1, factor=None):\n",
        "        self.Ip = Ip\n",
        "        self.use_gamma = use_gamma\n",
        "        self.factor = factor\n",
        "        super(SNConvolution2D, self).__init__(\n",
        "            in_channels, out_channels, ksize, stride, pad,\n",
        "            nobias, initialW, initial_bias)\n",
        "        self.u = np.random.normal(size=(1, out_channels)).astype(dtype=\"f\")\n",
        "        self.register_persistent('u')\n",
        "\n",
        "    @property\n",
        "    def W_bar(self):\n",
        "        \"\"\"\n",
        "        Spectrally Normalized Weight\n",
        "        \"\"\"\n",
        "        W_mat = self.W.reshape(self.W.shape[0], -1)\n",
        "        sigma, _u, _ = max_singular_value(W_mat, self.u, self.Ip)\n",
        "        if self.factor:\n",
        "            sigma = sigma / self.factor\n",
        "        sigma = broadcast_to(sigma.reshape((1, 1, 1, 1)), self.W.shape)\n",
        "        if chainer.config.train:\n",
        "            # Update estimated 1st singular vector\n",
        "            self.u[:] = _u\n",
        "        if hasattr(self, 'gamma'):\n",
        "            return broadcast_to(self.gamma, self.W.shape) * self.W / sigma\n",
        "        else:\n",
        "            return self.W / sigma\n",
        "\n",
        "    def _initialize_params(self, in_size):\n",
        "        super(SNConvolution2D, self)._initialize_params(in_size)\n",
        "        if self.use_gamma:\n",
        "            W_mat = self.W.data.reshape(self.W.shape[0], -1)\n",
        "            _, s, _ = np.linalg.svd(W_mat)\n",
        "            with self.init_scope():\n",
        "                self.gamma = chainer.Parameter(s[0], (1, 1, 1, 1))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Applies the convolution layer.\n",
        "        Args:\n",
        "            x (~chainer.Variable): Input image.\n",
        "        Returns:\n",
        "            ~chainer.Variable: Output of the convolution.\n",
        "        \"\"\"\n",
        "        if self.W.data is None:\n",
        "            self._initialize_params(x.shape[1])\n",
        "        return convolution_2d.convolution_2d(\n",
        "            x, self.W_bar, self.b, self.stride, self.pad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM4w8vMKS8in",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _downsample(x):\n",
        "    # Downsample (Mean Avg Pooling with 2x2 kernel)\n",
        "    return F.average_pooling_2d(x, 2)\n",
        "\n",
        "\n",
        "class DBlock(chainer.Chain):\n",
        "    def __init__(self, in_channels, out_channels, hidden_channels=None, ksize=3, pad=1,\n",
        "                 activation=F.relu, downsample=False):\n",
        "        super(DBlock, self).__init__()\n",
        "        initializer = chainer.initializers.GlorotUniform(math.sqrt(2))\n",
        "        initializer_sc = chainer.initializers.GlorotUniform()\n",
        "        self.activation = activation\n",
        "        self.downsample = downsample\n",
        "        self.learnable_sc = (in_channels != out_channels) or downsample\n",
        "        hidden_channels = in_channels if hidden_channels is None else hidden_channels\n",
        "        with self.init_scope():\n",
        "            self.c1 = SNConvolution2D(in_channels, hidden_channels, ksize=ksize, pad=pad, initialW=initializer)\n",
        "            self.c2 = SNConvolution2D(hidden_channels, out_channels, ksize=ksize, pad=pad, initialW=initializer)\n",
        "            if self.learnable_sc:\n",
        "                self.c_sc = SNConvolution2D(in_channels, out_channels, ksize=1, pad=0, initialW=initializer_sc)\n",
        "\n",
        "    def residual(self, x):\n",
        "        h = x\n",
        "        h = self.activation(h)\n",
        "        h = self.c1(h)\n",
        "        h = self.activation(h)\n",
        "        h = self.c2(h)\n",
        "        if self.downsample:\n",
        "            h = _downsample(h)\n",
        "        return h\n",
        "\n",
        "    def shortcut(self, x):\n",
        "        if self.learnable_sc:\n",
        "            x = self.c_sc(x)\n",
        "            if self.downsample:\n",
        "                return _downsample(x)\n",
        "            else:\n",
        "                return x\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.residual(x) + self.shortcut(x)\n",
        "\n",
        "\n",
        "class OptimizedBlock(chainer.Chain):\n",
        "    def __init__(self, in_channels, out_channels, ksize=3, pad=1, activation=F.relu):\n",
        "        super(OptimizedBlock, self).__init__()\n",
        "        initializer = chainer.initializers.GlorotUniform(math.sqrt(2))\n",
        "        initializer_sc = chainer.initializers.GlorotUniform()\n",
        "        self.activation = activation\n",
        "        with self.init_scope():\n",
        "            self.c1 = SNConvolution2D(in_channels, out_channels, ksize=ksize, pad=pad, initialW=initializer)\n",
        "            self.c2 = SNConvolution2D(out_channels, out_channels, ksize=ksize, pad=pad, initialW=initializer)\n",
        "            self.c_sc = SNConvolution2D(in_channels, out_channels, ksize=1, pad=0, initialW=initializer_sc)\n",
        "\n",
        "    def residual(self, x):\n",
        "        h = x\n",
        "        h = self.c1(h)\n",
        "        h = self.activation(h)\n",
        "        h = self.c2(h)\n",
        "        h = _downsample(h)\n",
        "        return h\n",
        "\n",
        "    def shortcut(self, x):\n",
        "        return self.c_sc(_downsample(x))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.residual(x) + self.shortcut(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lagog6cdQNzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_noise(h, sigma=0.2):\n",
        "    xp = cuda.get_array_module(h.data)\n",
        "    if chainer.config.train:\n",
        "        return h + sigma * xp.random.randn(*h.shape)\n",
        "    else:\n",
        "        return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY-831NFQMJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(chainer.Chain):\n",
        "\n",
        "    def __init__(self, bottom_width=4, bottom_height=2, ch=128, n_classes=10, wscale=0.02, activation=F.relu):\n",
        "        w = chainer.initializers.GlorotUniform()\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.activation = activation\n",
        "        with self.init_scope():\n",
        "            self.block1 = OptimizedBlock(3, ch)\n",
        "            self.block2 = DBlock(ch, ch, activation=activation, downsample=True)\n",
        "            self.block3 = DBlock(ch, ch, activation=activation, downsample=False)\n",
        "            self.block4 = DBlock(ch, ch, activation=activation, downsample=False)\n",
        "            self.block5 = DBlock(ch * 4, ch * 8, activation=activation, downsample=True)\n",
        "            self.block6 = DBlock(ch * 8, ch * 8, activation=activation, downsample=False)\n",
        "            self.l7 = SNLinear(ch, 1, initialW=w, nobias=True)\n",
        "            if n_classes > 0:\n",
        "                self.l_y = SNEmbedID(n_classes, ch, initialW=w)\n",
        "\n",
        "    def __call__(self, x, y = None):\n",
        "        h = x\n",
        "        h = self.block1(h)\n",
        "        h = self.block2(h)\n",
        "        h = self.block3(h)\n",
        "        h = self.block4(h)\n",
        "\n",
        "        #h = self.block5(h)\n",
        "        #h = self.block6(h)\n",
        "        h = self.activation(h)\n",
        "        \n",
        "        h = F.sum(h, axis=(2, 3))  # Global pooling\n",
        "        \n",
        "        output = self.l7(h)\n",
        "        \n",
        "        if y is not None:\n",
        "            w_y = self.l_y(y)\n",
        "            output += F.sum(w_y * h, axis=1, keepdims=True)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMBOImQjZeaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen = Generator(n_hidden=n_hidden, n_classes=0)\n",
        "dis = Discriminator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaZR2PeVZjcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup an optimizer\n",
        "def make_optimizer(model, alpha=0.0002, beta1=0.0, beta2=0.9):\n",
        "    optimizer = chainer.optimizers.Adam(alpha=alpha, beta1=beta1, beta2=beta2)\n",
        "    optimizer.setup(model)\n",
        "    return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNKJ1wPRQPZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt_gen = make_optimizer(gen)\n",
        "opt_dis = make_optimizer(dis)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjEb6qZrQ-uG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_hinge_dis(dis_fake, dis_real):\n",
        "    loss = F.mean(F.relu(1. - dis_real))\n",
        "    loss += F.mean(F.relu(1. + dis_fake))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def loss_hinge_gen(dis_fake):\n",
        "    loss = -F.mean(dis_fake)\n",
        "    return loss\n",
        "\n",
        "\n",
        "class DCGANUpdater(chainer.training.updaters.StandardUpdater):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.gen, self.dis = kwargs.pop('models')\n",
        "        super(DCGANUpdater, self).__init__(*args, **kwargs)\n",
        "        self.xp = self.gen.xp\n",
        "        self.loss_gen = loss_hinge_gen\n",
        "        self.loss_dis = loss_hinge_dis\n",
        "\n",
        "    def loss_dis(self, dis, y_fake, y_real):\n",
        "        batchsize = len(y_fake)\n",
        "        L1 = F.sum(F.softplus(-y_real)) /  batchsize\n",
        "        L2 = F.sum(F.softplus(y_fake)) /  batchsize\n",
        "        \n",
        "        loss = L1 + L2\n",
        "        chainer.report({'loss': loss}, dis)\n",
        "        return loss\n",
        "\n",
        "    def loss_gen(self, gen, y_fake):\n",
        "        batchsize = len(y_fake)\n",
        "        loss = F.sum(F.softplus(-y_fake)) /  batchsize\n",
        "        chainer.report({'loss': loss}, gen)\n",
        "        return loss\n",
        "    \n",
        "    def loss_hinge_dis(self, dis, dis_fake, dis_real):\n",
        "        batchsize = len(dis_fake)\n",
        "        loss = F.mean(F.relu(1. - dis_real))\n",
        "        loss += F.mean(F.relu(1. + dis_fake))\n",
        "        chainer.report({'loss': loss}, dis)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def loss_hinge_gen(self, gen, dis_fake):\n",
        "        batchsize = len(dis_fake)\n",
        "        loss = -F.mean(dis_fake)\n",
        "        chainer.report({'loss': loss}, gen)\n",
        "        return loss\n",
        "\n",
        "    def make_onehot(self, classes):\n",
        "        return self.xp.eye(10, dtype='f')[classes]\n",
        "\n",
        "    def update_core(self):\n",
        "        gen_optimizer = self.get_optimizer('gen')\n",
        "        dis_optimizer = self.get_optimizer('dis')\n",
        "        gen, dis = self.gen, self.dis\n",
        "        \n",
        "        for i in range(3):\n",
        "          if i == 0:\n",
        "            z = Variable(self.xp.asarray(gen.make_hidden(128 - 1)))\n",
        "            #onehot = self.xp.random.randint(low=0, high=10, size=(128)).astype(self.xp.int32)\n",
        "            x_fake = gen(z)\n",
        "            y_fake = dis(x_fake)\n",
        "            loss_gen = self.loss_gen(dis_fake=y_fake)\n",
        "            gen.cleargrads()\n",
        "            loss_gen.backward()\n",
        "            gen_optimizer.update()\n",
        "            chainer.reporter.report({'loss_gen': loss_gen})\n",
        "\n",
        "          batch = self.get_iterator('main').next()\n",
        "          batchsize = len(batch)\n",
        "          images = [batch[i][0] for i in range(batchsize)]\n",
        "          labels = [batch[i][1] for i in range(batchsize)]\n",
        "          x_2real = [np.concatenate((batch[i][0],batch[i+1][0]), axis=1) for i in range(batchsize -1)]\n",
        "          x_real = Variable(self.converter(x_2real, self.device)) / 255.\n",
        "\n",
        "          #x_label = Variable(self.converter(labels, self.device))\n",
        "\n",
        "          #xp_label = chainer.cuda.get_array_module(x_label.data)\n",
        "          #labels = xp_label.asarray(labels)\n",
        "\n",
        "          #classes = self.xp.random.random_integers(0, 9, len(batch))\n",
        "          #onehot = chainer.Variable(self.make_onehot(classes))\n",
        "          #classes = chainer.Variable(classes)\n",
        "\n",
        "          xp = chainer.backends.cuda.get_array_module(x_real.data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          y_real = dis(x_real)\n",
        "          x_fake = gen(z)\n",
        "          y_fake = dis(x_fake)\n",
        "          x_fake.unchain_backward()\n",
        "\n",
        "          loss_dis = self.loss_dis(dis_fake=y_fake, dis_real=y_real)\n",
        "          dis.cleargrads()\n",
        "          loss_dis.backward()\n",
        "          dis_optimizer.update()\n",
        "          chainer.reporter.report({'loss_dis': loss_dis})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          #dis_optimizer.update(self.loss_dis, dis, y_fake, y_real)\n",
        "          #gen_optimizer.update(self.loss_gen, gen, y_fake)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--UyAQApQWtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import chainer.backends.cuda\n",
        "\n",
        "\n",
        "def out_generated_image(gen, dis, rows, cols, seed, dst):\n",
        "    @chainer.training.make_extension()\n",
        "    def make_image(trainer):\n",
        "        np.random.seed(seed)\n",
        "        n_images = rows * cols\n",
        "        xp = gen.xp\n",
        "        z = Variable(xp.asarray(gen.make_hidden(n_images)))\n",
        "        with chainer.using_config('train', False):\n",
        "            x = gen(z)\n",
        "        x = chainer.backends.cuda.to_cpu(x.data)\n",
        "        np.random.seed()\n",
        "\n",
        "        x = np.asarray(np.clip(x * 255, 0.0, 255.0), dtype=np.uint8)\n",
        "        _, _, H, W = x.shape\n",
        "        x = x.reshape((rows, cols, 3, H, W))\n",
        "        x = x.transpose(0, 3, 1, 4, 2)\n",
        "        x = x.reshape((rows * H, cols * W, 3))\n",
        "\n",
        "        preview_dir = '{}/preview'.format(dst)\n",
        "        preview_path = preview_dir +\\\n",
        "            '/image{:0>8}.png'.format(trainer.updater.iteration)\n",
        "        if not os.path.exists(preview_dir):\n",
        "            os.makedirs(preview_dir)\n",
        "        Image.fromarray(x).save(preview_path)\n",
        "    return make_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbJnF8rKcxx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "updater = DCGANUpdater(\n",
        "      models=(gen, dis),\n",
        "      iterator=train_iter,\n",
        "      optimizer={\n",
        "          'gen': opt_gen, 'dis': opt_dis},\n",
        "      device=gpu_id)\n",
        "trainer = chainer.training.Trainer(updater, (n_epoch, 'epoch'), out=out_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddKM13U7QdFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "snapshot_interval = (snapshot_interval, 'iteration')\n",
        "display_interval = (display_interval, 'iteration')\n",
        "trainer.extend(\n",
        "    extensions.snapshot(filename='snapshot_iter_{.updater.iteration}.npz'),\n",
        "    trigger=snapshot_interval)\n",
        "trainer.extend(extensions.snapshot_object(\n",
        "    gen, 'gen_iter_{.updater.iteration}.npz'), trigger=snapshot_interval)\n",
        "trainer.extend(extensions.snapshot_object(\n",
        "    dis, 'dis_iter_{.updater.iteration}.npz'), trigger=snapshot_interval)\n",
        "trainer.extend(extensions.LogReport(trigger=display_interval))\n",
        "trainer.extend(extensions.PrintReport([\n",
        "    'epoch', 'iteration', 'loss_dis', 'loss_gen',\n",
        "]), trigger=display_interval)\n",
        "trainer.extend(extensions.ProgressBar(update_interval=100))\n",
        "trainer.extend(\n",
        "    out_generated_image(\n",
        "        gen, dis,\n",
        "        1, 10, seed, out_dir),\n",
        "    trigger=snapshot_interval)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOgCeR7tQlx9",
        "colab_type": "code",
        "outputId": "127ccd65-4b34-4891-e144-68d9c38e3c00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8534
        }
      },
      "source": [
        "# Run the training\n",
        "trainer.run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch       iteration   loss_dis    loss_gen  \n",
            "\u001b[J0           100         0.0114707   3.21905     \n",
            "\u001b[J     total [..................................................]  0.32%\n",
            "this epoch [################..................................] 32.00%\n",
            "       100 iter, 0 epoch / 100 epochs\n",
            "       inf iters/sec. Estimated time to finish: 0:00:00.\n",
            "\u001b[4A\u001b[J0           200         0.170875    3.15945     \n",
            "\u001b[J     total [..................................................]  0.64%\n",
            "this epoch [################################..................] 64.00%\n",
            "       200 iter, 0 epoch / 100 epochs\n",
            "   0.60065 iters/sec. Estimated time to finish: 14:21:33.791901.\n",
            "\u001b[4A\u001b[J0           300         0.156588    2.56536     \n",
            "\u001b[J     total [..................................................]  0.96%\n",
            "this epoch [################################################..] 96.00%\n",
            "       300 iter, 0 epoch / 100 epochs\n",
            "   0.60022 iters/sec. Estimated time to finish: 14:19:24.203232.\n",
            "\u001b[4A\u001b[J1           400         0.629684    1.89656     \n",
            "\u001b[J     total [..................................................]  1.28%\n",
            "this epoch [##############....................................] 28.00%\n",
            "       400 iter, 1 epoch / 100 epochs\n",
            "   0.59962 iters/sec. Estimated time to finish: 14:17:29.403200.\n",
            "\u001b[4A\u001b[J1           500         0.941176    1.13565     \n",
            "\u001b[J     total [..................................................]  1.60%\n",
            "this epoch [##############################....................] 60.00%\n",
            "       500 iter, 1 epoch / 100 epochs\n",
            "   0.59829 iters/sec. Estimated time to finish: 14:16:36.642700.\n",
            "\u001b[4A\u001b[J1           600         1.26648     0.920883    \n",
            "\u001b[J     total [..................................................]  1.92%\n",
            "this epoch [##############################################....] 92.00%\n",
            "       600 iter, 1 epoch / 100 epochs\n",
            "   0.59705 iters/sec. Estimated time to finish: 14:15:36.155447.\n",
            "\u001b[4A\u001b[J2           700         1.29197     0.742596    \n",
            "\u001b[J     total [#.................................................]  2.24%\n",
            "this epoch [############......................................] 24.00%\n",
            "       700 iter, 2 epoch / 100 epochs\n",
            "   0.59614 iters/sec. Estimated time to finish: 14:14:06.503616.\n",
            "\u001b[4A\u001b[J2           800         1.34361     1.18337     \n",
            "\u001b[J     total [#.................................................]  2.56%\n",
            "this epoch [############################......................] 56.00%\n",
            "       800 iter, 2 epoch / 100 epochs\n",
            "    0.5955 iters/sec. Estimated time to finish: 14:12:13.489593.\n",
            "\u001b[4A\u001b[J2           900         1.39751     0.769938    \n",
            "\u001b[J     total [#.................................................]  2.88%\n",
            "this epoch [###########################################.......] 88.00%\n",
            "       900 iter, 2 epoch / 100 epochs\n",
            "   0.59496 iters/sec. Estimated time to finish: 14:10:11.679488.\n",
            "\u001b[4A\u001b[J3           1000        1.4267      0.674457    \n",
            "\u001b[J     total [#.................................................]  3.20%\n",
            "this epoch [##########........................................] 20.00%\n",
            "      1000 iter, 3 epoch / 100 epochs\n",
            "   0.59455 iters/sec. Estimated time to finish: 14:07:58.399905.\n",
            "\u001b[4A\u001b[J3           1100        1.38966     0.663082    \n",
            "\u001b[J     total [#.................................................]  3.52%\n",
            "this epoch [##########################........................] 52.00%\n",
            "      1100 iter, 3 epoch / 100 epochs\n",
            "   0.58543 iters/sec. Estimated time to finish: 14:18:21.003041.\n",
            "\u001b[4A\u001b[J3           1200        1.43744     0.373242    \n",
            "\u001b[J     total [#.................................................]  3.84%\n",
            "this epoch [#########################################.........] 84.00%\n",
            "      1200 iter, 3 epoch / 100 epochs\n",
            "   0.58585 iters/sec. Estimated time to finish: 14:14:52.753966.\n",
            "\u001b[4A\u001b[J4           1300        1.41967     0.708597    \n",
            "\u001b[J     total [##................................................]  4.16%\n",
            "this epoch [########..........................................] 16.00%\n",
            "      1300 iter, 4 epoch / 100 epochs\n",
            "   0.58621 iters/sec. Estimated time to finish: 14:11:30.493946.\n",
            "\u001b[4A\u001b[J4           1400        1.51729     0.604564    \n",
            "\u001b[J     total [##................................................]  4.48%\n",
            "this epoch [########################..........................] 48.00%\n",
            "      1400 iter, 4 epoch / 100 epochs\n",
            "   0.58656 iters/sec. Estimated time to finish: 14:08:09.577419.\n",
            "\u001b[4A\u001b[J4           1500        1.52473     0.73401     \n",
            "\u001b[J     total [##................................................]  4.80%\n",
            "this epoch [#######################################...........] 80.00%\n",
            "      1500 iter, 4 epoch / 100 epochs\n",
            "   0.58689 iters/sec. Estimated time to finish: 14:04:50.678627.\n",
            "\u001b[4A\u001b[J5           1600        1.62829     0.525907    \n",
            "\u001b[J     total [##................................................]  5.12%\n",
            "this epoch [######............................................] 12.00%\n",
            "      1600 iter, 5 epoch / 100 epochs\n",
            "   0.58716 iters/sec. Estimated time to finish: 14:01:37.657249.\n",
            "\u001b[4A\u001b[J5           1700        1.49411     0.470619    \n",
            "\u001b[J     total [##................................................]  5.44%\n",
            "this epoch [######################............................] 44.00%\n",
            "      1700 iter, 5 epoch / 100 epochs\n",
            "   0.58742 iters/sec. Estimated time to finish: 13:58:24.395768.\n",
            "\u001b[4A\u001b[J5           1800        1.50723     0.418621    \n",
            "\u001b[J     total [##................................................]  5.76%\n",
            "this epoch [#####################################.............] 76.00%\n",
            "      1800 iter, 5 epoch / 100 epochs\n",
            "   0.58771 iters/sec. Estimated time to finish: 13:55:09.561384.\n",
            "\u001b[4A\u001b[J6           1900        1.56238     0.200339    \n",
            "\u001b[J     total [###...............................................]  6.08%\n",
            "this epoch [####..............................................]  8.00%\n",
            "      1900 iter, 6 epoch / 100 epochs\n",
            "   0.58794 iters/sec. Estimated time to finish: 13:52:00.076236.\n",
            "\u001b[4A\u001b[J6           2000        1.62857     0.253037    \n",
            "\u001b[J     total [###...............................................]  6.40%\n",
            "this epoch [####################..............................] 40.00%\n",
            "      2000 iter, 6 epoch / 100 epochs\n",
            "   0.58818 iters/sec. Estimated time to finish: 13:48:49.682480.\n",
            "\u001b[4A\u001b[J6           2100        1.52557     0.397247    \n",
            "\u001b[J     total [###...............................................]  6.72%\n",
            "this epoch [###################################...............] 72.00%\n",
            "      2100 iter, 6 epoch / 100 epochs\n",
            "   0.58428 iters/sec. Estimated time to finish: 13:51:30.051857.\n",
            "\u001b[4A\u001b[J7           2200        1.53393     0.402709    \n",
            "\u001b[J     total [###...............................................]  7.04%\n",
            "this epoch [##................................................]  4.00%\n",
            "      2200 iter, 7 epoch / 100 epochs\n",
            "   0.58471 iters/sec. Estimated time to finish: 13:48:02.983825.\n",
            "\u001b[4A\u001b[J7           2300        1.55332     0.246194    \n",
            "\u001b[J     total [###...............................................]  7.36%\n",
            "this epoch [##################................................] 36.00%\n",
            "      2300 iter, 7 epoch / 100 epochs\n",
            "   0.58502 iters/sec. Estimated time to finish: 13:44:45.134071.\n",
            "\u001b[4A\u001b[J7           2400        1.7147      0.174582    \n",
            "\u001b[J     total [###...............................................]  7.68%\n",
            "this epoch [#################################.................] 68.00%\n",
            "      2400 iter, 7 epoch / 100 epochs\n",
            "   0.58536 iters/sec. Estimated time to finish: 13:41:25.673931.\n",
            "\u001b[4A\u001b[J8           2500        1.45107     0.507069    \n",
            "\u001b[J     total [####..............................................]  8.00%\n",
            "this epoch [..................................................]  0.00%\n",
            "      2500 iter, 8 epoch / 100 epochs\n",
            "   0.58571 iters/sec. Estimated time to finish: 13:38:05.803249.\n",
            "\u001b[4A\u001b[J8           2600        1.60104     0.289613    \n",
            "\u001b[J     total [####..............................................]  8.32%\n",
            "this epoch [################..................................] 32.00%\n",
            "      2600 iter, 8 epoch / 100 epochs\n",
            "     0.586 iters/sec. Estimated time to finish: 13:34:50.897873.\n",
            "\u001b[4A\u001b[J8           2700        1.61784     0.322628    \n",
            "\u001b[J     total [####..............................................]  8.64%\n",
            "this epoch [################################..................] 64.00%\n",
            "      2700 iter, 8 epoch / 100 epochs\n",
            "   0.58626 iters/sec. Estimated time to finish: 13:31:38.224634.\n",
            "\u001b[4A\u001b[J8           2800        1.53126     0.598863    \n",
            "\u001b[J     total [####..............................................]  8.96%\n",
            "this epoch [################################################..] 96.00%\n",
            "      2800 iter, 8 epoch / 100 epochs\n",
            "   0.58654 iters/sec. Estimated time to finish: 13:28:24.955371.\n",
            "\u001b[4A\u001b[J9           2900        1.69235     0.330907    \n",
            "\u001b[J     total [####..............................................]  9.28%\n",
            "this epoch [#############.....................................] 28.00%\n",
            "      2900 iter, 9 epoch / 100 epochs\n",
            "   0.58678 iters/sec. Estimated time to finish: 13:25:14.563571.\n",
            "\u001b[4A\u001b[J9           3000        1.58042     0.399709    \n",
            "\u001b[J     total [####..............................................]  9.60%\n",
            "this epoch [#############################.....................] 60.00%\n",
            "      3000 iter, 9 epoch / 100 epochs\n",
            "   0.58699 iters/sec. Estimated time to finish: 13:22:07.056933.\n",
            "\u001b[4A\u001b[J9           3100        1.55418     0.44573     \n",
            "\u001b[J     total [####..............................................]  9.92%\n",
            "this epoch [##############################################....] 92.00%\n",
            "      3100 iter, 9 epoch / 100 epochs\n",
            "   0.58448 iters/sec. Estimated time to finish: 13:22:42.829416.\n",
            "\u001b[4A\u001b[J10          3200        1.58757     0.267039    \n",
            "\u001b[J     total [#####.............................................] 10.24%\n",
            "this epoch [############......................................] 24.00%\n",
            "      3200 iter, 10 epoch / 100 epochs\n",
            "   0.58474 iters/sec. Estimated time to finish: 13:19:29.977333.\n",
            "\u001b[4A\u001b[J10          3300        1.59426     0.290306    \n",
            "\u001b[J     total [#####.............................................] 10.56%\n",
            "this epoch [############################......................] 56.00%\n",
            "      3300 iter, 10 epoch / 100 epochs\n",
            "   0.58504 iters/sec. Estimated time to finish: 13:16:14.791739.\n",
            "\u001b[4A\u001b[J10          3400        1.66156     0.325106    \n",
            "\u001b[J     total [#####.............................................] 10.88%\n",
            "this epoch [############################################......] 88.00%\n",
            "      3400 iter, 10 epoch / 100 epochs\n",
            "   0.58526 iters/sec. Estimated time to finish: 13:13:05.394204.\n",
            "\u001b[4A\u001b[J11          3500        1.56783     0.271405    \n",
            "\u001b[J     total [#####.............................................] 11.20%\n",
            "this epoch [#########.........................................] 20.00%\n",
            "      3500 iter, 11 epoch / 100 epochs\n",
            "   0.58547 iters/sec. Estimated time to finish: 13:09:57.757049.\n",
            "\u001b[4A\u001b[J11          3600        1.65587     0.310025    \n",
            "\u001b[J     total [#####.............................................] 11.52%\n",
            "this epoch [#########################.........................] 52.00%\n",
            "      3600 iter, 11 epoch / 100 epochs\n",
            "    0.5857 iters/sec. Estimated time to finish: 13:06:48.522330.\n",
            "\u001b[4A\u001b[J11          3700        1.64238     0.441119    \n",
            "\u001b[J     total [#####.............................................] 11.84%\n",
            "this epoch [#########################################.........] 84.00%\n",
            "      3700 iter, 11 epoch / 100 epochs\n",
            "   0.58592 iters/sec. Estimated time to finish: 13:03:39.991749.\n",
            "\u001b[4A\u001b[J12          3800        1.66141     0.186483    \n",
            "\u001b[J     total [######............................................] 12.16%\n",
            "this epoch [########..........................................] 16.00%\n",
            "      3800 iter, 12 epoch / 100 epochs\n",
            "   0.58608 iters/sec. Estimated time to finish: 13:00:36.283417.\n",
            "\u001b[4A\u001b[J12          3900        1.69446     0.297232    \n",
            "\u001b[J     total [######............................................] 12.48%\n",
            "this epoch [########################..........................] 48.00%\n",
            "      3900 iter, 12 epoch / 100 epochs\n",
            "   0.58629 iters/sec. Estimated time to finish: 12:57:28.936358.\n",
            "\u001b[4A\u001b[J12          4000        1.64433     0.286284    \n",
            "\u001b[J     total [######............................................] 12.80%\n",
            "this epoch [########################################..........] 80.00%\n",
            "      4000 iter, 12 epoch / 100 epochs\n",
            "    0.5865 iters/sec. Estimated time to finish: 12:54:22.164075.\n",
            "\u001b[4A\u001b[J13          4100        1.68301     0.189463    \n",
            "\u001b[J     total [######............................................] 13.12%\n",
            "this epoch [#####.............................................] 12.00%\n",
            "      4100 iter, 13 epoch / 100 epochs\n",
            "   0.58463 iters/sec. Estimated time to finish: 12:53:59.605494.\n",
            "\u001b[4A\u001b[J13          4200        1.75184     0.418006    \n",
            "\u001b[J     total [######............................................] 13.44%\n",
            "this epoch [#####################.............................] 44.00%\n",
            "      4200 iter, 13 epoch / 100 epochs\n",
            "   0.58483 iters/sec. Estimated time to finish: 12:50:52.891370.\n",
            "\u001b[4A\u001b[J13          4300        1.66855     0.281393    \n",
            "\u001b[J     total [######............................................] 13.76%\n",
            "this epoch [#####################################.............] 76.00%\n",
            "      4300 iter, 13 epoch / 100 epochs\n",
            "   0.58505 iters/sec. Estimated time to finish: 12:47:44.831645.\n",
            "\u001b[4A\u001b[J14          4400        1.58557     0.612734    \n",
            "\u001b[J     total [#######...........................................] 14.08%\n",
            "this epoch [####..............................................]  8.00%\n",
            "      4400 iter, 14 epoch / 100 epochs\n",
            "   0.58526 iters/sec. Estimated time to finish: 12:44:37.314861.\n",
            "\u001b[4A\u001b[J14          4500        1.70241     0.260788    \n",
            "\u001b[J     total [#######...........................................] 14.40%\n",
            "this epoch [####################..............................] 40.00%\n",
            "      4500 iter, 14 epoch / 100 epochs\n",
            "   0.58545 iters/sec. Estimated time to finish: 12:41:31.552608.\n",
            "\u001b[4A\u001b[J14          4600        1.63159     0.397123    \n",
            "\u001b[J     total [#######...........................................] 14.72%\n",
            "this epoch [####################################..............] 72.00%\n",
            "      4600 iter, 14 epoch / 100 epochs\n",
            "   0.58563 iters/sec. Estimated time to finish: 12:38:26.729381.\n",
            "\u001b[4A\u001b[J15          4700        1.7463      0.353602    \n",
            "\u001b[J     total [#######...........................................] 15.04%\n",
            "this epoch [#.................................................]  4.00%\n",
            "      4700 iter, 15 epoch / 100 epochs\n",
            "   0.58579 iters/sec. Estimated time to finish: 12:35:23.628635.\n",
            "\u001b[4A\u001b[J15          4800        1.70892     0.186373    \n",
            "\u001b[J     total [#######...........................................] 15.36%\n",
            "this epoch [#################.................................] 36.00%\n",
            "      4800 iter, 15 epoch / 100 epochs\n",
            "   0.58598 iters/sec. Estimated time to finish: 12:32:18.094083.\n",
            "\u001b[4A\u001b[J15          4900        1.65381     0.292187    \n",
            "\u001b[J     total [#######...........................................] 15.68%\n",
            "this epoch [#################################.................] 68.00%\n",
            "      4900 iter, 15 epoch / 100 epochs\n",
            "   0.58612 iters/sec. Estimated time to finish: 12:29:16.770805.\n",
            "\u001b[4A\u001b[J16          5000        1.71943     0.267726    \n",
            "\u001b[J     total [########..........................................] 16.00%\n",
            "this epoch [..................................................]  0.00%\n",
            "      5000 iter, 16 epoch / 100 epochs\n",
            "   0.58627 iters/sec. Estimated time to finish: 12:26:14.272682.\n",
            "\u001b[4A\u001b[J16          5100        1.67625     0.254389    \n",
            "\u001b[J     total [########..........................................] 16.32%\n",
            "this epoch [################..................................] 32.00%\n",
            "      5100 iter, 16 epoch / 100 epochs\n",
            "   0.58478 iters/sec. Estimated time to finish: 12:25:17.997849.\n",
            "\u001b[4A\u001b[J16          5200        1.6685      0.395668    \n",
            "\u001b[J     total [########..........................................] 16.64%\n",
            "this epoch [################################..................] 64.00%\n",
            "      5200 iter, 16 epoch / 100 epochs\n",
            "   0.58493 iters/sec. Estimated time to finish: 12:22:15.150912.\n",
            "\u001b[4A\u001b[J16          5300        1.75939     0.338695    \n",
            "\u001b[J     total [########..........................................] 16.96%\n",
            "this epoch [################################################..] 96.00%\n",
            "      5300 iter, 16 epoch / 100 epochs\n",
            "   0.58512 iters/sec. Estimated time to finish: 12:19:10.027581.\n",
            "\u001b[4A\u001b[J17          5400        1.72637     0.405012    \n",
            "\u001b[J     total [########..........................................] 17.28%\n",
            "this epoch [##############....................................] 28.00%\n",
            "      5400 iter, 17 epoch / 100 epochs\n",
            "   0.58526 iters/sec. Estimated time to finish: 12:16:08.204950.\n",
            "\u001b[4A\u001b[J17          5500        1.6896      0.232813    \n",
            "\u001b[J     total [########..........................................] 17.60%\n",
            "this epoch [##############################....................] 60.00%\n",
            "      5500 iter, 17 epoch / 100 epochs\n",
            "   0.58544 iters/sec. Estimated time to finish: 12:13:04.383138.\n",
            "\u001b[4A\u001b[J17          5600        1.76042     -0.0040229  \n",
            "\u001b[J     total [########..........................................] 17.92%\n",
            "this epoch [##############################################....] 92.00%\n",
            "      5600 iter, 17 epoch / 100 epochs\n",
            "    0.5856 iters/sec. Estimated time to finish: 12:10:01.255746.\n",
            "\u001b[4A\u001b[J18          5700        1.74114     0.400787    \n",
            "\u001b[J     total [#########.........................................] 18.24%\n",
            "this epoch [###########.......................................] 24.00%\n",
            "      5700 iter, 18 epoch / 100 epochs\n",
            "   0.58575 iters/sec. Estimated time to finish: 12:06:59.619103.\n",
            "\u001b[4A\u001b[J18          5800        1.70281     0.45288     \n",
            "\u001b[J     total [#########.........................................] 18.56%\n",
            "this epoch [###########################.......................] 56.00%\n",
            "      5800 iter, 18 epoch / 100 epochs\n",
            "   0.58589 iters/sec. Estimated time to finish: 12:03:58.331083.\n",
            "\u001b[4A\u001b[J18          5900        1.78043     0.250785    \n",
            "\u001b[J     total [#########.........................................] 18.88%\n",
            "this epoch [###########################################.......] 88.00%\n",
            "      5900 iter, 18 epoch / 100 epochs\n",
            "   0.58602 iters/sec. Estimated time to finish: 12:00:57.968663.\n",
            "\u001b[4A\u001b[J19          6000        1.74261     0.412034    \n",
            "\u001b[J     total [#########.........................................] 19.20%\n",
            "this epoch [#########.........................................] 20.00%\n",
            "      6000 iter, 19 epoch / 100 epochs\n",
            "   0.58617 iters/sec. Estimated time to finish: 11:57:56.202644.\n",
            "\u001b[4A\u001b[J19          6100        1.74082     0.144836    \n",
            "\u001b[J     total [#########.........................................] 19.52%\n",
            "this epoch [#########################.........................] 52.00%\n",
            "      6100 iter, 19 epoch / 100 epochs\n",
            "   0.58495 iters/sec. Estimated time to finish: 11:56:35.337260.\n",
            "\u001b[4A\u001b[J19          6200        1.71281     0.475859    \n",
            "\u001b[J     total [#########.........................................] 19.84%\n",
            "this epoch [#########################################.........] 84.00%\n",
            "      6200 iter, 19 epoch / 100 epochs\n",
            "   0.58509 iters/sec. Estimated time to finish: 11:53:33.739989.\n",
            "\u001b[4A\u001b[J20          6300        1.80847     0.040408    \n",
            "\u001b[J     total [##########........................................] 20.16%\n",
            "this epoch [########..........................................] 16.00%\n",
            "      6300 iter, 20 epoch / 100 epochs\n",
            "   0.58525 iters/sec. Estimated time to finish: 11:50:31.694809.\n",
            "\u001b[4A\u001b[J20          6400        1.79024     0.146279    \n",
            "\u001b[J     total [##########........................................] 20.48%\n",
            "this epoch [########################..........................] 48.00%\n",
            "      6400 iter, 20 epoch / 100 epochs\n",
            "   0.58537 iters/sec. Estimated time to finish: 11:47:31.684701.\n",
            "\u001b[4A\u001b[J20          6500        1.74756     0.320381    \n",
            "\u001b[J     total [##########........................................] 20.80%\n",
            "this epoch [########################################..........] 80.00%\n",
            "      6500 iter, 20 epoch / 100 epochs\n",
            "   0.58551 iters/sec. Estimated time to finish: 11:44:30.750088.\n",
            "\u001b[4A\u001b[J21          6600        1.70719     0.443815    \n",
            "\u001b[J     total [##########........................................] 21.12%\n",
            "this epoch [######............................................] 12.00%\n",
            "      6600 iter, 21 epoch / 100 epochs\n",
            "   0.58563 iters/sec. Estimated time to finish: 11:41:31.365005.\n",
            "\u001b[4A\u001b[J21          6700        1.7574      0.352931    \n",
            "\u001b[J     total [##########........................................] 21.44%\n",
            "this epoch [######################............................] 44.00%\n",
            "      6700 iter, 21 epoch / 100 epochs\n",
            "   0.58577 iters/sec. Estimated time to finish: 11:38:30.751775.\n",
            "\u001b[4A\u001b[J21          6800        1.77167     0.19992     \n",
            "\u001b[J     total [##########........................................] 21.76%\n",
            "this epoch [######################################............] 76.00%\n",
            "      6800 iter, 21 epoch / 100 epochs\n",
            "   0.58588 iters/sec. Estimated time to finish: 11:35:32.205326.\n",
            "\u001b[4A\u001b[J22          6900        1.73736     0.0866385   \n",
            "\u001b[J     total [###########.......................................] 22.08%\n",
            "this epoch [###...............................................]  8.00%\n",
            "      6900 iter, 22 epoch / 100 epochs\n",
            "   0.58599 iters/sec. Estimated time to finish: 11:32:33.821536.\n",
            "\u001b[4A\u001b[J22          7000        1.82214     0.186804    \n",
            "\u001b[J     total [###########.......................................] 22.40%\n",
            "this epoch [###################...............................] 40.00%\n",
            "      7000 iter, 22 epoch / 100 epochs\n",
            "   0.58612 iters/sec. Estimated time to finish: 11:29:33.639694.\n",
            "\u001b[4A\u001b[J22          7100        1.77258     0.138963    \n",
            "\u001b[J     total [###########.......................................] 22.72%\n",
            "this epoch [###################################...............] 72.00%\n",
            "      7100 iter, 22 epoch / 100 epochs\n",
            "   0.58507 iters/sec. Estimated time to finish: 11:27:57.334276.\n",
            "\u001b[4A\u001b[J23          7200        1.78562     0.28779     \n",
            "\u001b[J     total [###########.......................................] 23.04%\n",
            "this epoch [#.................................................]  4.00%\n",
            "      7200 iter, 23 epoch / 100 epochs\n",
            "   0.58518 iters/sec. Estimated time to finish: 11:24:58.155641.\n",
            "\u001b[4A\u001b[J23          7300        1.76922     0.0148424   \n",
            "\u001b[J     total [###########.......................................] 23.36%\n",
            "this epoch [#################.................................] 36.00%\n",
            "      7300 iter, 23 epoch / 100 epochs\n",
            "   0.58531 iters/sec. Estimated time to finish: 11:21:58.693476.\n",
            "\u001b[4A\u001b[J23          7400        1.74694     0.282162    \n",
            "\u001b[J     total [###########.......................................] 23.68%\n",
            "this epoch [#################################.................] 68.00%\n",
            "      7400 iter, 23 epoch / 100 epochs\n",
            "   0.58543 iters/sec. Estimated time to finish: 11:18:59.171235.\n",
            "\u001b[4A\u001b[J24          7500        1.73053     0.136836    \n",
            "\u001b[J     total [############......................................] 24.00%\n",
            "this epoch [..................................................]  0.00%\n",
            "      7500 iter, 24 epoch / 100 epochs\n",
            "   0.58555 iters/sec. Estimated time to finish: 11:15:59.985529.\n",
            "\u001b[4A\u001b[J24          7600        1.81716     0.20026     \n",
            "\u001b[J     total [############......................................] 24.32%\n",
            "this epoch [################..................................] 32.00%\n",
            "      7600 iter, 24 epoch / 100 epochs\n",
            "   0.58566 iters/sec. Estimated time to finish: 11:13:01.631230.\n",
            "\u001b[4A\u001b[J24          7700        1.79987     0.205348    \n",
            "\u001b[J     total [############......................................] 24.64%\n",
            "this epoch [################################..................] 64.00%\n",
            "      7700 iter, 24 epoch / 100 epochs\n",
            "   0.58577 iters/sec. Estimated time to finish: 11:10:03.574479.\n",
            "\u001b[4A\u001b[J24          7800        1.79117     0.168318    \n",
            "\u001b[J     total [############......................................] 24.96%\n",
            "this epoch [################################################..] 96.00%\n",
            "      7800 iter, 24 epoch / 100 epochs\n",
            "   0.58587 iters/sec. Estimated time to finish: 11:07:05.690108.\n",
            "\u001b[4A\u001b[J25          7900        1.70495     0.273393    \n",
            "\u001b[J     total [############......................................] 25.28%\n",
            "this epoch [##############....................................] 28.00%\n",
            "      7900 iter, 25 epoch / 100 epochs\n",
            "   0.58598 iters/sec. Estimated time to finish: 11:04:07.691767.\n",
            "\u001b[4A\u001b[J25          8000        1.73315     0.169106    \n",
            "\u001b[J     total [############......................................] 25.60%\n",
            "this epoch [##############################....................] 60.00%\n",
            "      8000 iter, 25 epoch / 100 epochs\n",
            "   0.58608 iters/sec. Estimated time to finish: 11:01:10.271798.\n",
            "\u001b[4A\u001b[J25          8100        1.76716     0.169745    \n",
            "\u001b[J     total [############......................................] 25.92%\n",
            "this epoch [##############################################....] 92.00%\n",
            "      8100 iter, 25 epoch / 100 epochs\n",
            "   0.58513 iters/sec. Estimated time to finish: 10:59:23.991410.\n",
            "\u001b[4A\u001b[J26          8200        1.81667     0.322192    \n",
            "\u001b[J     total [#############.....................................] 26.24%\n",
            "this epoch [###########.......................................] 24.00%\n",
            "      8200 iter, 26 epoch / 100 epochs\n",
            "   0.58523 iters/sec. Estimated time to finish: 10:56:26.077670.\n",
            "\u001b[4A\u001b[J26          8300        1.76283     0.297889    \n",
            "\u001b[J     total [#############.....................................] 26.56%\n",
            "this epoch [###########################.......................] 56.00%\n",
            "      8300 iter, 26 epoch / 100 epochs\n",
            "   0.58533 iters/sec. Estimated time to finish: 10:53:28.836442.\n",
            "\u001b[4A\u001b[J26          8400        1.83028     0.230934    \n",
            "\u001b[J     total [#############.....................................] 26.88%\n",
            "this epoch [###########################################.......] 88.00%\n",
            "      8400 iter, 26 epoch / 100 epochs\n",
            "   0.58543 iters/sec. Estimated time to finish: 10:50:31.184600.\n",
            "\u001b[4A\u001b[J27          8500        1.79447     0.193954    \n",
            "\u001b[J     total [#############.....................................] 27.20%\n",
            "this epoch [#########.........................................] 20.00%\n",
            "      8500 iter, 27 epoch / 100 epochs\n",
            "   0.58554 iters/sec. Estimated time to finish: 10:47:33.113725.\n",
            "\u001b[4A\u001b[J27          8600        1.73076     0.242329    \n",
            "\u001b[J     total [#############.....................................] 27.52%\n",
            "this epoch [#########################.........................] 52.00%\n",
            "      8600 iter, 27 epoch / 100 epochs\n",
            "   0.58564 iters/sec. Estimated time to finish: 10:44:35.892097.\n",
            "\u001b[4A\u001b[J27          8700        1.74634     0.22312     \n",
            "\u001b[J     total [#############.....................................] 27.84%\n",
            "this epoch [#########################################.........] 84.00%\n",
            "      8700 iter, 27 epoch / 100 epochs\n",
            "   0.58573 iters/sec. Estimated time to finish: 10:41:39.143872.\n",
            "\u001b[4A\u001b[J28          8800        1.757       0.0899517   \n",
            "\u001b[J     total [##############....................................] 28.16%\n",
            "this epoch [########..........................................] 16.00%\n",
            "      8800 iter, 28 epoch / 100 epochs\n",
            "   0.58582 iters/sec. Estimated time to finish: 10:38:42.388357.\n",
            "\u001b[4A\u001b[J28          8900        1.78721     -0.0405917  \n",
            "\u001b[J     total [##############....................................] 28.48%\n",
            "this epoch [########################..........................] 48.00%\n",
            "      8900 iter, 28 epoch / 100 epochs\n",
            "   0.58591 iters/sec. Estimated time to finish: 10:35:45.691993.\n",
            "\u001b[4A\u001b[J28          9000        1.77137     0.168429    \n",
            "\u001b[J     total [##############....................................] 28.80%\n",
            "this epoch [########################################..........] 80.00%\n",
            "      9000 iter, 28 epoch / 100 epochs\n",
            "     0.586 iters/sec. Estimated time to finish: 10:32:49.217543.\n",
            "\u001b[4A\u001b[J29          9100        1.75315     0.181508    \n",
            "\u001b[J     total [##############....................................] 29.12%\n",
            "this epoch [######............................................] 12.00%\n",
            "      9100 iter, 29 epoch / 100 epochs\n",
            "   0.58513 iters/sec. Estimated time to finish: 10:30:54.606696.\n",
            "\u001b[4A\u001b[J29          9200        1.77171     0.127462    \n",
            "\u001b[J     total [##############....................................] 29.44%\n",
            "this epoch [######################............................] 44.00%\n",
            "      9200 iter, 29 epoch / 100 epochs\n",
            "   0.58523 iters/sec. Estimated time to finish: 10:27:57.389070.\n",
            "\u001b[4A\u001b[J29          9300        1.78038     0.135851    \n",
            "\u001b[J     total [##############....................................] 29.76%\n",
            "this epoch [######################################............] 76.00%\n",
            "      9300 iter, 29 epoch / 100 epochs\n",
            "   0.58532 iters/sec. Estimated time to finish: 10:25:01.076107.\n",
            "\u001b[4A\u001b[J30          9400        1.74215     0.210026    \n",
            "\u001b[J     total [###############...................................] 30.08%\n",
            "this epoch [###...............................................]  8.00%\n",
            "      9400 iter, 30 epoch / 100 epochs\n",
            "    0.5854 iters/sec. Estimated time to finish: 10:22:04.952724.\n",
            "\u001b[4A\u001b[J30          9500        1.81724     0.180922    \n",
            "\u001b[J     total [###############...................................] 30.40%\n",
            "this epoch [###################...............................] 40.00%\n",
            "      9500 iter, 30 epoch / 100 epochs\n",
            "   0.58548 iters/sec. Estimated time to finish: 10:19:08.694420.\n",
            "\u001b[4A\u001b[J30          9600        1.76192     0.058321    \n",
            "\u001b[J     total [###############...................................] 30.72%\n",
            "this epoch [###################################...............] 72.00%\n",
            "      9600 iter, 30 epoch / 100 epochs\n",
            "   0.58558 iters/sec. Estimated time to finish: 10:16:11.897690.\n",
            "\u001b[4A\u001b[J31          9700        1.68372     0.430924    \n",
            "\u001b[J     total [###############...................................] 31.04%\n",
            "this epoch [#.................................................]  4.00%\n",
            "      9700 iter, 31 epoch / 100 epochs\n",
            "   0.58567 iters/sec. Estimated time to finish: 10:13:15.650069.\n",
            "\u001b[4A\u001b[J31          9800        1.83949     0.159265    \n",
            "\u001b[J     total [###############...................................] 31.36%\n",
            "this epoch [#################.................................] 36.00%\n",
            "      9800 iter, 31 epoch / 100 epochs\n",
            "   0.58575 iters/sec. Estimated time to finish: 10:10:19.588498.\n",
            "\u001b[4A\u001b[J31          9900        1.73834     -0.0848668  \n",
            "\u001b[J     total [###############...................................] 31.68%\n",
            "this epoch [#################################.................] 68.00%\n",
            "      9900 iter, 31 epoch / 100 epochs\n",
            "   0.58583 iters/sec. Estimated time to finish: 10:07:24.018634.\n",
            "\u001b[4A\u001b[J32          10000       1.76583     0.175487    \n",
            "\u001b[J     total [################..................................] 32.00%\n",
            "this epoch [..................................................]  0.00%\n",
            "     10000 iter, 32 epoch / 100 epochs\n",
            "   0.58591 iters/sec. Estimated time to finish: 10:04:28.298097.\n",
            "\u001b[4A"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whbljy_pwfpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image, display_png\n",
        "import glob\n",
        "\n",
        "image_files = sorted(glob.glob(out_dir + '/preview/*.png'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hMW0qYmwkVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_png(Image(image_files[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3PWjcUjwljh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_png(Image(image_files[-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}